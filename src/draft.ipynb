{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1a1c64-d8f0-459e-8aa3-90a4a990aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53749a8-6f87-4db3-b16c-3468ae9f4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d38e74e2-56ed-493d-83a3-fa93d46ff5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/initial_data/frmgham2_project_data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8bce6e-dc3f-4e50-8cd3-f90dcdcdf00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11627, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de48fb5-1683-41be-9b88-eab7098c9600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b44d44-f680-4032-b8da-673988533153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a37d6e6-85ca-4ad3-a254-0247b68c8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smote_variants import MWMOTE\n",
    "import re\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c4735b-68e6-4d64-a07b-67e3baff2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8231fcf-ba5d-4ff4-a6ac-e9a9e85ead7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary:logistic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Implementation of the scikit-learn API for XGBoost classification.\n",
       "See :doc:`/python/sklearn_estimator` for more information.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : Optional[int]\n",
       "        Number of boosting rounds.\n",
       "\n",
       "    max_depth :  Optional[int]\n",
       "        Maximum tree depth for base learners.\n",
       "    max_leaves :\n",
       "        Maximum number of leaves; 0 indicates no limit.\n",
       "    max_bin :\n",
       "        If using histogram-based algorithm, maximum number of bins per feature\n",
       "    grow_policy :\n",
       "        Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
       "        depth-wise. 1: favor splitting at nodes with highest loss change.\n",
       "    learning_rate : Optional[float]\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "    verbosity : Optional[int]\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "        Specify the learning task and the corresponding learning objective or\n",
       "        a custom objective function to be used (see note below).\n",
       "    booster: Optional[str]\n",
       "        Specify which booster to use: gbtree, gblinear or dart.\n",
       "    tree_method: Optional[str]\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
       "        default, XGBoost will choose the most conservative option available.  It's\n",
       "        recommended to study this option from the parameters document :doc:`tree method\n",
       "        </treemethod>`\n",
       "    n_jobs : Optional[int]\n",
       "        Number of parallel threads used to run xgboost.  When used with other\n",
       "        Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
       "        parallelize and balance the threads.  Creating thread contention will\n",
       "        significantly slow down both algorithms.\n",
       "    gamma : Optional[float]\n",
       "        (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
       "        leaf node of the tree.\n",
       "    min_child_weight : Optional[float]\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "    max_delta_step : Optional[float]\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "    subsample : Optional[float]\n",
       "        Subsample ratio of the training instance.\n",
       "    sampling_method :\n",
       "        Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
       "          - ``uniform``: select random training instances uniformly.\n",
       "          - ``gradient_based`` select random training instances with higher probability\n",
       "            when the gradient and hessian are larger. (cf. CatBoost)\n",
       "    colsample_bytree : Optional[float]\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "    colsample_bylevel : Optional[float]\n",
       "        Subsample ratio of columns for each level.\n",
       "    colsample_bynode : Optional[float]\n",
       "        Subsample ratio of columns for each split.\n",
       "    reg_alpha : Optional[float]\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "    reg_lambda : Optional[float]\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "    scale_pos_weight : Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "    base_score : Optional[float]\n",
       "        The initial prediction score of all instances, global bias.\n",
       "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float, default np.nan\n",
       "        Value in the data which needs to be present as a missing value.\n",
       "    num_parallel_tree: Optional[int]\n",
       "        Used for boosting random forest.\n",
       "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
       "        Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
       "        for more information.\n",
       "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
       "        3, 4]]``, where each inner list is a group of indices of features that are\n",
       "        allowed to interact with each other.  See :doc:`tutorial\n",
       "        </tutorials/feature_interaction_constraint>` for more information\n",
       "    importance_type: Optional[str]\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
       "          without bias.\n",
       "\n",
       "    device : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
       "\n",
       "    validate_parameters : Optional[bool]\n",
       "\n",
       "        Give warnings for unknown parameter.\n",
       "\n",
       "    enable_categorical : bool\n",
       "\n",
       "        .. versionadded:: 1.5.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
       "        should be used to specify categorical data type.  Also, JSON/UBJSON\n",
       "        serialization format is required.\n",
       "\n",
       "    feature_types : Optional[FeatureTypes]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        Used for specifying feature types without constructing a dataframe. See\n",
       "        :py:class:`DMatrix` for details.\n",
       "\n",
       "    max_cat_to_onehot : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
       "        for categorical data.  When number of categories is lesser than the threshold\n",
       "        then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
       "        into children nodes. Also, `enable_categorical` needs to be set to have\n",
       "        categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    max_cat_threshold : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.7.0\n",
       "\n",
       "        .. note:: This parameter is experimental\n",
       "\n",
       "        Maximum number of categories considered for each split. Used only by\n",
       "        partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
       "        needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
       "        </tutorials/categorical>` and :ref:`cat-param` for details.\n",
       "\n",
       "    multi_strategy : Optional[str]\n",
       "\n",
       "        .. versionadded:: 2.0.0\n",
       "\n",
       "        .. note:: This parameter is working-in-progress.\n",
       "\n",
       "        The strategy used for training multi-target models, including multi-target\n",
       "        regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
       "        more information.\n",
       "\n",
       "        - ``one_output_per_tree``: One model for each target.\n",
       "        - ``multi_output_tree``:  Use multi-target trees.\n",
       "\n",
       "    eval_metric : Optional[Union[str, List[str], Callable]]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        Metric used for monitoring the training result and early stopping.  It can be a\n",
       "        string or list of strings as names of predefined metric in XGBoost (See\n",
       "        doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
       "        user defined metric that looks like `sklearn.metrics`.\n",
       "\n",
       "        If custom objective is also provided, then custom metric should implement the\n",
       "        corresponding reverse link function.\n",
       "\n",
       "        Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
       "        object is provided, it's assumed to be a cost function and by default XGBoost will\n",
       "        minimize the result during early stopping.\n",
       "\n",
       "        For advanced usage on Early stopping like directly choosing to maximize instead of\n",
       "        minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
       "        for more.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "             This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old\n",
       "             one receives un-transformed prediction regardless of whether custom\n",
       "             objective is being used.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from sklearn.datasets import load_diabetes\n",
       "            from sklearn.metrics import mean_absolute_error\n",
       "            X, y = load_diabetes(return_X_y=True)\n",
       "            reg = xgb.XGBRegressor(\n",
       "                tree_method=\"hist\",\n",
       "                eval_metric=mean_absolute_error,\n",
       "            )\n",
       "            reg.fit(X, y, eval_set=[(X, y)])\n",
       "\n",
       "    early_stopping_rounds : Optional[int]\n",
       "\n",
       "        .. versionadded:: 1.6.0\n",
       "\n",
       "        - Activates early stopping. Validation metric needs to improve at least once in\n",
       "          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
       "          least one item in **eval_set** in :py:meth:`fit`.\n",
       "\n",
       "        - If early stopping occurs, the model will have two additional attributes:\n",
       "          :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
       "          :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
       "          number of trees during inference. If users want to access the full model\n",
       "          (including trees built after early stopping), they can specify the\n",
       "          `iteration_range` in these inference methods. In addition, other utilities\n",
       "          like model plotting can also use the entire model.\n",
       "\n",
       "        - If you prefer to discard the trees after `best_iteration`, consider using the\n",
       "          callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
       "\n",
       "        - If there's more than one item in **eval_set**, the last entry will be used for\n",
       "          early stopping.  If there's more than one metric in **eval_metric**, the last\n",
       "          metric will be used for early stopping.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "            This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
       "\n",
       "    callbacks : Optional[List[TrainingCallback]]\n",
       "        List of callback functions that are applied at end of each iteration.\n",
       "        It is possible to use predefined callbacks by using\n",
       "        :ref:`Callback API <callback_api>`.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           States in callback are not preserved during training, which means callback\n",
       "           objects can not be reused for multiple training sessions without\n",
       "           reinitialization or deepcopy.\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            for params in parameters_grid:\n",
       "                # be sure to (re)initialize the callbacks before each run\n",
       "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
       "                reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
       "                reg.fit(X, y)\n",
       "\n",
       "    kwargs : dict, optional\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
       "        can be found :doc:`here </parameter>`.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature\n",
       "            ``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/ml_env/lib/python3.11/site-packages/xgboost/sklearn.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     XGBRFClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XGBClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f477b-54a2-4a77-b4fb-0f41ffbdec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [50,100,150] \n",
    "learning_rate = [.1,.04]\n",
    "maximum tree depth (max_depth), \n",
    "minimum child weight (min_child_weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8ae3d-e98e-4ff6-9a46-c216bbe6f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03cb309c-e69f-4cea-9655-87e082e4ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"../data/initial_data/frmgham2_project_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3628e44-b29c-443a-b6f6-20f7213f0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1[df_1['PERIOD'] == 3].drop(\"PERIOD\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77dd22f3-894a-42e4-ae9e-532ed466a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv(\"../data/initial_data/frmgham2_period_3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c68ef63a-981f-4689-906c-45daf5ce9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"../data/initial_data/frmgham2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f830443d-45fb-4510-aa66-41962e041f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "# model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10474c-faea-412d-abc9-17831cb9bda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_course_kernel",
   "language": "python",
   "name": "ml_course_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
