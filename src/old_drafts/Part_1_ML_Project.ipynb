{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"Downloads/frmgham2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341eefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_percentages(df):\n",
    "    total_count = df.shape[0]  \n",
    "    percentages = pd.DataFrame(index=df.columns, columns=['Null Values %', 'Zero Values %', 'Non-Null Values %'])\n",
    "\n",
    "    for column in df.columns:\n",
    "        null_count = df[column].isna().sum()\n",
    "        zero_count = (df[column] == 0).sum()\n",
    "        non_null_count = df[column].notna().sum()\n",
    "\n",
    "        percentages.loc[column, 'Null Values %'] = (null_count / total_count) * 100\n",
    "        percentages.loc[column, 'Zero Values %'] = (zero_count / total_count) * 100\n",
    "        percentages.loc[column, 'Non-Null Values %'] = (non_null_count / total_count) * 100\n",
    "\n",
    "    return percentages\n",
    "\n",
    "percentages = calculate_value_percentages(df)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229c731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf755477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of interest\n",
    "hdlc = df['HDLC']\n",
    "ldlc = df['LDLC']\n",
    "\n",
    "correlation_hdlc = df.corrwith(hdlc)\n",
    "correlation_ldlc = df.corrwith(ldlc)\n",
    "\n",
    "print(\"Correlation of HDLC with other columns:\")\n",
    "print(correlation_hdlc)\n",
    "print(\"\\nCorrelation of LDLC with other columns:\")\n",
    "print(correlation_ldlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for period 3 only\n",
    "df_period_3 = df[df['PERIOD'] == 3]\n",
    "\n",
    "correlation_hdlc = df_period_3.drop('PERIOD', axis=1).corrwith(df_period_3['HDLC'])\n",
    "correlation_ldlc = df_period_3.drop('PERIOD', axis=1).corrwith(df_period_3['LDLC'])\n",
    "\n",
    "print(\"Correlation of HDLC with other columns in Period 3:\")\n",
    "print(correlation_hdlc)\n",
    "print(\"\\nCorrelation of LDLC with other columns in Period 3:\")\n",
    "print(correlation_ldlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('HDLC', axis=1)\n",
    "#df = df.drop('LDLC', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4461bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics for the numeric columns\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c91a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Histograms for numeric variables\n",
    "numeric_columns = ['TOTCHOL', 'AGE', 'SYSBP', 'DIABP', 'BMI', 'HEARTRTE', 'GLUCOSE', 'TIME']\n",
    "fig, axes = plt.subplots(len(numeric_columns), 1, figsize=(8, 5 * len(numeric_columns)))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.histplot(df[col], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a57706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for comparing distributions across categories\n",
    "categorical_columns = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS', 'PREVCHD', 'PREVAP', 'PREVMI', 'PREVSTRK', 'PREVHYP', 'DEATH', 'ANGINA', 'HOSPMI', 'MI_FCHD', 'ANYCHD', 'STROKE', 'CVD', 'HYPERTEN']\n",
    "fig, axes = plt.subplots(len(categorical_columns), 1, figsize=(8, 5 * len(categorical_columns)))\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    sns.countplot(x=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'Count of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe121d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of the numeric features\n",
    "corr_matrix = df[numeric_columns].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation matrix of numeric features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8495101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots to observe relationships between key features and a target variable (e.g., SYSBP vs AGE)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='AGE', y='SYSBP', hue='DEATH', data=df)\n",
    "plt.title('Age vs. Systolic Blood Pressure colored by Death')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right skewed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daae69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assuming 'feature_name' is right-skewed\n",
    "df['TOTCHOL'] = np.log1p(df['TOTCHOL'])\n",
    "df['SYSBP'] = np.log1p(df['SYSBP'])\n",
    "df['BMI'] = np.log1p(df['BMI'])\n",
    "df['HEARTRTE'] = np.log1p(df['HEARTRTE'])\n",
    "df['GLUCOSE'] = np.log1p(df['GLUCOSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7491ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histograms for numeric variables\n",
    "numeric_columns = ['TOTCHOL', 'AGE', 'SYSBP', 'DIABP', 'BMI', 'HEARTRTE', 'GLUCOSE', 'TIME']\n",
    "fig, axes = plt.subplots(len(numeric_columns), 1, figsize=(8, 5 * len(numeric_columns)))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.histplot(df[col], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5782c",
   "metadata": {},
   "source": [
    "# building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Handle missing values (you might need more complex strategies depending on your data)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables as necessary\n",
    "le = LabelEncoder()\n",
    "categorical_features = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS', 'PREVHYP', \n",
    "                        'PREVAP','PREVMI','PREVSTRK', 'PREVHYP', 'DEATH','ANGINA','HOSPMI',\n",
    "                        'MI_FCHD','ANYCHD','STROKE','HYPERTEN']  \n",
    "for col in categorical_features:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cdff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = df.drop('CVD', axis=1)\n",
    "y = df['CVD']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Scale features (important for models like logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79992ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Models dictionary\n",
    "models = {'Decision Tree': dt, 'Random Forest': rf, 'Logistic Regression': lr, 'Naive Bayes': nb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae644494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    g_measure = (2 * precision * recall) / (precision + recall)\n",
    "    youdens_stat = recall - (fp / (fp + tn))\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"G-Measure:\", g_measure)\n",
    "    print(\"Youden’s Statistic:\", youdens_stat)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    evaluate_model(model, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Using a simpler decision tree model with depth limitation\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Cross-validation to check for consistency across different subsets\n",
    "scores = cross_val_score(dt, X_train_scaled, y_train, cv=5)\n",
    "print(\"Cross-Validated Accuracy Scores:\", scores)\n",
    "\n",
    "# Evaluate model again\n",
    "evaluate_model(dt, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93159d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Only transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now you can use X_train_scaled and X_test_scaled for model training and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the models have been imported and initialized as before\n",
    "\n",
    "# Retrain models with normalized data\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    print(f\"\\n{name}:\")\n",
    "    evaluate_model(model, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'CVD' is the target variable\n",
    "X = df.drop('CVD', axis=1)\n",
    "y = df['CVD']\n",
    "\n",
    "# Encoding categorical variables if not already encoded\n",
    "# Adjust the handling of categorical variables based on your dataset specifics\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "tree_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extracting feature importances\n",
    "feature_importances = tree_classifier.feature_importances_\n",
    "\n",
    "# Creating a DataFrame to visualize them\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=features_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Information Gain')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7605559",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab98e8",
   "metadata": {},
   "source": [
    "# Missing value using GAN deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a3e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"Downloads/frmgham2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed27bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " RANDID         0\n",
      "SEX            0\n",
      "TOTCHOL      409\n",
      "AGE            0\n",
      "SYSBP          0\n",
      "DIABP          0\n",
      "CURSMOKE       0\n",
      "CIGPDAY       79\n",
      "BMI           52\n",
      "DIABETES       0\n",
      "BPMEDS       593\n",
      "HEARTRTE       6\n",
      "GLUCOSE     1440\n",
      "educ         295\n",
      "PREVCHD        0\n",
      "PREVAP         0\n",
      "PREVMI         0\n",
      "PREVSTRK       0\n",
      "PREVHYP        0\n",
      "TIME           0\n",
      "PERIOD         0\n",
      "HDLC        8600\n",
      "LDLC        8601\n",
      "DEATH          0\n",
      "ANGINA         0\n",
      "HOSPMI         0\n",
      "MI_FCHD        0\n",
      "ANYCHD         0\n",
      "STROKE         0\n",
      "CVD            0\n",
      "HYPERTEN       0\n",
      "TIMEAP         0\n",
      "TIMEMI         0\n",
      "TIMEMIFC       0\n",
      "TIMECHD        0\n",
      "TIMESTRK       0\n",
      "TIMECVD        0\n",
      "TIMEDTH        0\n",
      "TIMEHYP        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594749bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:32:22.454969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:615: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:615: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 0.1472 - val_loss: 0.1198\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 0.0769\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.0615\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.0512\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.0442\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.0387\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0316 - val_loss: 0.0341\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0277 - val_loss: 0.0304\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0271\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0239\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0189 - val_loss: 0.0213\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0167 - val_loss: 0.0190\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0148 - val_loss: 0.0170\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0132 - val_loss: 0.0149\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0118 - val_loss: 0.0136\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0089 - val_loss: 0.0102\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0082 - val_loss: 0.0093\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0086\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0070\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0065\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0048\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0037\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9656b13970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Prepare data (excluding the target variable if it's included)\n",
    "# Normalize features for better performance of the autoencoder\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df.drop(columns=['CVD']))  # Assuming 'CVD' is the target column\n",
    "\n",
    "# Replacing NaNs with the mean of the column for initial input\n",
    "df_scaled = np.where(np.isnan(df_scaled), np.ma.array(df_scaled, mask=np.isnan(df_scaled)).mean(axis=0), df_scaled)\n",
    "\n",
    "# Define the size of the encoded representations\n",
    "encoding_dim = 32  # or less, based on the complexity of your data\n",
    "\n",
    "# Input Layer\n",
    "input_layer = Input(shape=(df_scaled.shape[1],))\n",
    "# Encoder Layers\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# Decoder Layers\n",
    "decoded = Dense(df_scaled.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the autoencoder\n",
    "autoencoder.fit(df_scaled, df_scaled,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8237e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 1.49405772e-01, ...,\n",
       "        7.34428474e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.73174873e-01, ...,\n",
       "        7.34428474e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "       [3.79118892e-04, 1.00000000e+00, 2.42784380e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.51103565e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.79237965e-01],\n",
       "       [1.00000000e+00, 1.00000000e+00, 2.25806452e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.79237965e-01],\n",
       "       [1.00000000e+00, 1.00000000e+00, 2.27779996e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.79237965e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2af50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the autoencoder to predict the missing data\n",
    "predicted = autoencoder.predict(df_scaled)\n",
    "\n",
    "# Inverse transform the predicted data to original scale\n",
    "df_imputed = scaler.inverse_transform(predicted)\n",
    "\n",
    "# Assuming 'CVD' is your target and you're not imputing it\n",
    "columns_without_target = [col for col in df.columns if col != 'CVD']\n",
    "\n",
    "# Creating a DataFrame from the imputed data using the same columns that were used for imputation\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=columns_without_target)\n",
    "\n",
    "# Add the target column back from the original DataFrame if necessary\n",
    "df_imputed['CVD'] = df['CVD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b8a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RANDID', 'SEX', 'TOTCHOL', 'AGE', 'SYSBP', 'DIABP', 'CURSMOKE',\n",
      "       'CIGPDAY', 'BMI', 'DIABETES', 'BPMEDS', 'HEARTRTE', 'GLUCOSE', 'educ',\n",
      "       'PREVCHD', 'PREVAP', 'PREVMI', 'PREVSTRK', 'PREVHYP', 'TIME', 'PERIOD',\n",
      "       'HDLC', 'LDLC', 'DEATH', 'ANGINA', 'HOSPMI', 'MI_FCHD', 'ANYCHD',\n",
      "       'STROKE', 'HYPERTEN', 'TIMEAP', 'TIMEMI', 'TIMEMIFC', 'TIMECHD',\n",
      "       'TIMESTRK', 'TIMECVD', 'TIMEDTH', 'TIMEHYP', 'CVD'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_imputed.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4b916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RANDID', 'SEX', 'TOTCHOL', 'AGE', 'SYSBP', 'DIABP', 'CURSMOKE',\n",
      "       'CIGPDAY', 'BMI', 'DIABETES', 'BPMEDS', 'HEARTRTE', 'GLUCOSE', 'educ',\n",
      "       'PREVCHD', 'PREVAP', 'PREVMI', 'PREVSTRK', 'PREVHYP', 'TIME', 'PERIOD',\n",
      "       'HDLC', 'LDLC', 'DEATH', 'ANGINA', 'HOSPMI', 'MI_FCHD', 'ANYCHD',\n",
      "       'STROKE', 'CVD', 'HYPERTEN', 'TIMEAP', 'TIMEMI', 'TIMEMIFC', 'TIMECHD',\n",
      "       'TIMESTRK', 'TIMECVD', 'TIMEDTH', 'TIMEHYP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05895c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace only the missing values in the original DataFrame\n",
    "for column in df.columns:\n",
    "    df.loc[df[column].isnull(), column] = df_imputed[column]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e9fe0",
   "metadata": {},
   "source": [
    "Note:\n",
    "- The performance of an autoencoder for imputation largely depends on the nature of the dataset and the pattern of missingness. If the missing data is not random (Missing Not at Random - MNAR), the imputation might introduce bias.\n",
    "- It's important to test the imputed values against simple techniques (like mean, median imputation) to see if the complexity of an autoencoder is justified.\n",
    "- This approach gives you a sophisticated method for handling missing data, potentially preserving and learning from the underlying patterns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355be064",
   "metadata": {},
   "source": [
    "# Again building the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb2972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Handle missing values (you might need more complex strategies depending on your data)\n",
    "# df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables as necessary\n",
    "le = LabelEncoder()\n",
    "categorical_features = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS', 'PREVHYP', \n",
    "                        'PREVAP','PREVMI','PREVSTRK', 'PREVHYP', 'DEATH','ANGINA','HOSPMI',\n",
    "                        'MI_FCHD','ANYCHD','STROKE','HYPERTEN']  \n",
    "for col in categorical_features:\n",
    "    df_imputed[col] = le.fit_transform(df_imputed[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9a3f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:615: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:615: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:615: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "# Split data into features and target\n",
    "X = df_imputed.drop('CVD', axis=1)\n",
    "y = df_imputed['CVD']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Scale features (important for models like logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c1efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshulyadav/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Models dictionary\n",
    "models = {'Decision Tree': dt, 'Random Forest': rf, 'Logistic Regression': lr, 'Naive Bayes': nb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c3b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier\n",
      "Accuracy: 0.9768047954130832\n",
      "F1 Score: 0.9552988448016072\n",
      "G-Measure: 0.9552988448016072\n",
      "Youden’s Statistic: 0.9424195837023727\n",
      "MCC: 0.9396560395653762\n",
      "\n",
      "\n",
      "Model: RandomForestClassifier\n",
      "Accuracy: 0.983059682043263\n",
      "F1 Score: 0.9673202614379085\n",
      "G-Measure: 0.9673202614379085\n",
      "Youden’s Statistic: 0.9580872972706572\n",
      "MCC: 0.9558976421291869\n",
      "\n",
      "\n",
      "Model: LogisticRegression\n",
      "Accuracy: 0.9682043262965859\n",
      "F1 Score: 0.9375\n",
      "G-Measure: 0.9375\n",
      "Youden’s Statistic: 0.9071467522530604\n",
      "MCC: 0.9163801864085225\n",
      "\n",
      "\n",
      "Model: GaussianNB\n",
      "Accuracy: 0.8259056554599948\n",
      "F1 Score: 0.6776061776061776\n",
      "G-Measure: 0.6776061776061776\n",
      "Youden’s Statistic: 0.5752060178996776\n",
      "MCC: 0.5596612557345386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    g_measure = (2 * precision * recall) / (precision + recall)\n",
    "    youdens_stat = recall - (fp / (fp + tn))\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"G-Measure:\", g_measure)\n",
    "    print(\"Youden’s Statistic:\", youdens_stat)\n",
    "    print(\"MCC:\", mcc)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    evaluate_model(model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0596d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
